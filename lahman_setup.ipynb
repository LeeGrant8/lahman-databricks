{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b677a09b-81b7-4bc8-af8b-dff4e8831ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create schema + storage folders\n",
    "raw_dir   = \"/FileStore/lahman/raw\"\n",
    "delta_dir = \"/FileStore/lahman/delta\"\n",
    "\n",
    "# Create a dedicated schema for our tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lahman\")\n",
    "spark.sql(\"USE lahman\")\n",
    "\n",
    "# Create DBFS folders for raw CSVs and Delta outputs\n",
    "dbutils.fs.mkdirs(raw_dir)\n",
    "dbutils.fs.mkdirs(delta_dir)\n",
    "\n",
    "print(\"Database set to:\", spark.catalog.currentDatabase())\n",
    "display(dbutils.fs.ls(\"/FileStore/lahman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c0dc1c-56ba-4413-9f6a-da6c8e5bac10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files we just uploaded\n",
    "display(dbutils.fs.ls(\"/FileStore/lahman\"))\n",
    "\n",
    "# (optional) capture the zip path\n",
    "zip_paths = [f.path for f in dbutils.fs.ls(\"/FileStore/lahman\") if f.name.lower().endswith(\".zip\")]\n",
    "print(\"ZIPs found:\", zip_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adc57ee-86eb-4789-b5e4-b7ba2abae2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "\n",
    "zip_dir   = \"/tmp/lahman_zip\"\n",
    "raw_dir   = \"/FileStore/lahman/raw\"\n",
    "zip_paths = [f.path.replace(\"dbfs:\",\"/dbfs\") for f in dbutils.fs.ls(\"/FileStore/lahman\") if f.name.lower().endswith(\".zip\")]\n",
    "\n",
    "# Prep folders\n",
    "dbutils.fs.mkdirs(raw_dir)\n",
    "os.makedirs(zip_dir, exist_ok=True)\n",
    "\n",
    "# Extract first ZIP we find\n",
    "assert zip_paths, \"No ZIP found in /FileStore/lahman\"\n",
    "zip_path_local = zip_paths[0]\n",
    "print(\"Extracting:\", zip_path_local)\n",
    "\n",
    "with zipfile.ZipFile(zip_path_local, \"r\") as zf:\n",
    "    zf.extractall(zip_dir)\n",
    "\n",
    "# Copy only .csv files into DBFS raw folder\n",
    "count = 0\n",
    "for root, _, files in os.walk(zip_dir):\n",
    "    for fn in files:\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            src = os.path.join(root, fn)\n",
    "            dst = f\"{raw_dir}/{fn}\"\n",
    "            dbutils.fs.cp(f\"file:{src}\", dst, True)\n",
    "            count += 1\n",
    "\n",
    "print(f\"Copied {count} CSV files to {raw_dir}\")\n",
    "display(dbutils.fs.ls(raw_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dee8d16-de17-4081-a668-da538c85ac63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw_dir   = \"/FileStore/lahman/raw\"\n",
    "delta_dir = \"/FileStore/lahman/delta\"\n",
    "\n",
    "# Ensure database exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lahman\")\n",
    "spark.sql(\"USE lahman\")\n",
    "\n",
    "# Helper to make valid table names\n",
    "def clean_name(s: str) -> str:\n",
    "    name = os.path.splitext(s)[0]          # drop .csv\n",
    "    name = re.sub(r'[^A-Za-z0-9_]', '_', name)\n",
    "    return name.lower()\n",
    "\n",
    "# Read options for messy CSVs\n",
    "read_opts = {\n",
    "    \"header\": \"true\",\n",
    "    \"inferSchema\": \"true\",\n",
    "    \"multiLine\": \"true\",\n",
    "    \"escape\": \"\\\"\",\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "}\n",
    "\n",
    "# Loop files -> Delta -> Tables\n",
    "files = [f for f in dbutils.fs.ls(raw_dir) if f.name.lower().endswith(\".csv\")]\n",
    "assert files, f\"No CSVs found in {raw_dir}\"\n",
    "\n",
    "for f in files:\n",
    "    tbl = clean_name(f.name)\n",
    "    src = f\"{raw_dir}/{f.name}\"\n",
    "    dst = f\"{delta_dir}/{tbl}\"\n",
    "\n",
    "    df = spark.read.options(**read_opts).csv(src)\n",
    "\n",
    "    # Write Delta (overwrite so the cell is idempotent)\n",
    "    (df.write.format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"overwriteSchema\", \"true\")\n",
    "       .save(dst))\n",
    "\n",
    "    # Register external table at that location\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS lahman.{tbl}\n",
    "        USING DELTA\n",
    "        LOCATION '{dst}'\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Loaded tables:\")\n",
    "display(spark.sql(\"SHOW TABLES IN lahman\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9de693-cff6-43fa-8064-d725d4858b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE lahman;\n",
    "\n",
    "-- Top 20 home run seasons (Batting)\n",
    "SELECT playerID, yearID, teamID, G, AB, H, HR, RBI\n",
    "FROM batting\n",
    "ORDER BY HR DESC, yearID DESC\n",
    "LIMIT 20;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7899138254998121,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lahman_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
